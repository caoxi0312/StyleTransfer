{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1xtIGxI4vJJY",
    "outputId": "8a5e544e-5166-4fda-a056-e388cbfe652b"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "!pip3 install d2lzh\n",
    "!pip3 install mxnet\n",
    "import d2lzh as d2l\n",
    "from mxnet import autograd, gluon, image, init, nd, gpu\n",
    "from mxnet.gluon.data.vision import datasets, transforms\n",
    "from mxnet.gluon import model_zoo, nn\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 249
    },
    "id": "aa3WOoyZx_f5",
    "outputId": "fe5ec543-8de4-42e2-9277-45f578ac66aa"
   },
   "outputs": [],
   "source": [
    "d2l.set_figsize()\n",
    "content_img = image.imread('input_1.jpg')\n",
    "d2l.plt.imshow(content_img.asnumpy());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 247
    },
    "id": "JPK3leSwvQPN",
    "outputId": "081bcf40-4668-47d8-9ba4-cb8b37359b1c"
   },
   "outputs": [],
   "source": [
    "style_img = image.imread('3.jpg')\n",
    "d2l.plt.imshow(style_img.asnumpy());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zAMRIbtdz5Rn"
   },
   "outputs": [],
   "source": [
    "# Count the data in the imagenet\n",
    "rgb_mean = nd.array([0.485, 0.456, 0.406])\n",
    "rgb_std = nd.array([0.229, 0.224, 0.225])\n",
    "# Transfer to input type\n",
    "def preprocess(img, image_shape):\n",
    "  img = image.imresize(img, *image_shape)\n",
    "  img = (img.astype('float32') / 255 - rgb_mean) / rgb_std \n",
    "  return img.transpose((2, 0, 1)).expand_dims(axis=0)\n",
    "# Restore\n",
    "def postprocess(img):\n",
    "  img = img[0].as_in_context(rgb_std.context)\n",
    "  return (img.transpose((1, 2, 0)) * rgb_std + rgb_mean).clip(0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8xCMgeRX0vkm",
    "outputId": "2529eeaf-9f28-41eb-b8f9-f2c619a52a90"
   },
   "outputs": [],
   "source": [
    "from mxnet.gluon.model_zoo import vision as models\n",
    "# VGG-19 model\n",
    "pretrained_net = model_zoo.vision.vgg19(pretrained=True)\n",
    "pretrained_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LCZpjbii1i0-"
   },
   "outputs": [],
   "source": [
    "# Note down the corresponding number in the blocks and we only need the output for those layers.\n",
    "style_layers, content_layers = [0, 1, 2, 3, 5, 6, 10, 11, 19,20, 21,22,23,24,25], [25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wD0uh2cm1wR8"
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "for i in range(max(content_layers + style_layers) + 1):\n",
    "  net.add(pretrained_net.features[i]) # Remove the unwanted layers behind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gw-g070M2FFI"
   },
   "outputs": [],
   "source": [
    "# Given an inputx for each layer, put into the X\n",
    "#overwriteoverwrite x if is the needed layer，put the result into styles content\n",
    "def extract_features(X, content_layers, style_layers): \n",
    "  contents = []\n",
    "  styles = []\n",
    "  for i in range(len(net)):\n",
    "    X = net[i](X)\n",
    "    if i in style_layers:\n",
    "      styles.append(X) \n",
    "    if i in content_layers:\n",
    "      contents.append(X) \n",
    "  return contents, styles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yLFtEbTq3MBb"
   },
   "outputs": [],
   "source": [
    "def get_contents(image_shape, ctx):\n",
    "  content_X = preprocess(content_img, image_shape).copyto(ctx)\n",
    "  contents_Y, _ = extract_features(content_X, content_layers, style_layers) # 只要contents\n",
    "  return content_X, contents_Y\n",
    "def get_styles(image_shape, ctx):\n",
    "  style_X = preprocess(style_img, image_shape).copyto(ctx)\n",
    "  _, styles_Y = extract_features(style_X, content_layers, style_layers) # 只要style\n",
    "  return style_X, styles_Y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfLvEDOh3Wxg"
   },
   "source": [
    "# loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TTQ92xCN4rzv"
   },
   "outputs": [],
   "source": [
    "def content_loss(Y_hat, Y):\n",
    "  return (Y_hat - Y).square().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yYBRG4G94xe8"
   },
   "outputs": [],
   "source": [
    "def gram(X):\n",
    "  num_channels, n = X.shape[1], X.size // X.shape[1] # shape[1]:channel, 0:batch\n",
    "  X = X.reshape((num_channels, n))\n",
    "  return nd.dot(X, X.T) / (num_channels * n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aP3XwUlM5oFN"
   },
   "outputs": [],
   "source": [
    "def style_loss(Y_hat, gram_Y):\n",
    "  return (gram(Y_hat) - gram_Y).square().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pGHX5_eH5wv5"
   },
   "outputs": [],
   "source": [
    "# Reduce white noise by making the pixel values as close as possible to the collar similar\n",
    "def tv_loss(Y_hat):\n",
    "  return 0.5 * ((Y_hat[:, :, 1:, :] - Y_hat[:, :, :-1, :]).abs().mean() + \n",
    "                (Y_hat[:, :, :, 1:] - Y_hat[:, :, :, :-1]).abs().mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_62NOLsk58PF"
   },
   "outputs": [],
   "source": [
    "# channels = [net[l].weight.shape[0] for l in style_layers]\n",
    "# style_weight = [1e4/n**2 for n in channels] #Make the style more match color, adjustable, \n",
    "#the larger the closer to the style\n",
    "style_weight = 1e4\n",
    "content_weight = 1\n",
    "tv_weight = 10 # The larger the value, the smoother the image\n",
    "def compute_loss(X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram): \n",
    "  # 分别计算内容损失、样式损失和总变差损失\n",
    "  contents_l = [content_loss(Y_hat, Y) * content_weight for Y_hat, Y in zip(contents_Y_hat, contents_Y)]\n",
    "  styles_l = [style_loss(Y_hat, Y) * style_weight for Y_hat, Y in zip( styles_Y_hat, styles_Y_gram)]\n",
    "  tv_l = tv_loss(X) * tv_weight\n",
    "  # 对所有损失求和\n",
    "  l = nd.add_n(*styles_l) + nd.add_n(*contents_l) + tv_l \n",
    "  return contents_l, styles_l, tv_l, l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAb7jzQB6vEf"
   },
   "source": [
    "# Create and initialize the image graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hq2HkSf26wQl"
   },
   "outputs": [],
   "source": [
    "class GeneratedImage(nn.Block):\n",
    "  def __init__(self, img_shape, **kwargs):\n",
    "    super(GeneratedImage, self).__init__(**kwargs)\n",
    "    self.weight = self.params.get('weight', shape=img_shape)\n",
    "  def forward(self):\n",
    "    return self.weight.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O5ofgfkJ92mD"
   },
   "outputs": [],
   "source": [
    "def get_inits(X, ctx, lr, styles_Y): \n",
    "  gen_img = GeneratedImage(X.shape)\n",
    "  gen_img.initialize(init.Constant(X), ctx=ctx, force_reinit=True)\n",
    "  trainer = gluon.Trainer(gen_img.collect_params(), 'adam',{'learning_rate': lr}) \n",
    "  styles_Y_gram = [gram(Y) for Y in styles_Y]\n",
    "  return gen_img(), styles_Y_gram, trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFBUHer8-V_W"
   },
   "source": [
    "训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4qOO9vBQ-XSd"
   },
   "outputs": [],
   "source": [
    "def train(X, contents_Y, styles_Y, ctx, lr, max_epochs, lr_decay_epoch): \n",
    "  X, styles_Y_gram, trainer = get_inits(X, ctx, lr, styles_Y)\n",
    "  for i in range(max_epochs):\n",
    "    start = time.time() \n",
    "    with autograd.record():\n",
    "      contents_Y_hat, styles_Y_hat = extract_features(X, content_layers, style_layers)\n",
    "      contents_l, styles_l, tv_l, l = compute_loss(X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram)\n",
    "    l.backward() \n",
    "    trainer.step(1) \n",
    "    nd.waitall()\n",
    "    if i % 50 == 0 and i != 0:\n",
    "      print('epoch %3d, content loss %.2f, style loss %.2f, ' 'TV loss %.2f, %.2f sec'\n",
    "             % (i, nd.add_n(*contents_l).asscalar(),\n",
    "                nd.add_n(*styles_l).asscalar(), tv_l.asscalar(),\n",
    "                time.time() - start))\n",
    "    if i % lr_decay_epoch == 0 and i != 0:\n",
    "      trainer.set_learning_rate(trainer.learning_rate * 0.1)\n",
    "      print('change lr to %.1e' % trainer.learning_rate)\n",
    "  plt.imshow(postprocess(X).asnumpy()) \n",
    "  plt.show()\n",
    "  return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "5UGDRzTw_Uu3",
    "outputId": "a372547f-446b-4089-9ae9-288f244f2c99"
   },
   "outputs": [],
   "source": [
    "ctx, image_shape = d2l.try_gpu(), (225, 150)\n",
    "net.collect_params().reset_ctx(ctx)\n",
    "content_X, contents_Y = get_contents(image_shape, ctx)\n",
    "_, styles_Y = get_styles(image_shape, ctx)\n",
    "output = train(content_X, contents_Y, styles_Y, ctx, 0.01, 100, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1806
    },
    "id": "C6oH_LdcAQbe",
    "outputId": "c8aecfc6-989a-4afd-b7e2-2af4199b4230"
   },
   "outputs": [],
   "source": [
    "image_shape = (450, 300)\n",
    "_, content_Y = get_contents(image_shape, ctx)\n",
    "_, style_Y = get_styles(image_shape, ctx)\n",
    "X = preprocess(postprocess(output) * 255, image_shape)\n",
    "output = train(X, content_Y, style_Y, ctx, 0.01, 1000, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Jp-LSt7ukhD"
   },
   "outputs": [],
   "source": [
    "d2l.plt.imsave('neural-style-3.png', postprocess(output).asnumpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Style_Transfer_mxnet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_mxnet_latest_p37",
   "language": "python",
   "name": "conda_mxnet_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
